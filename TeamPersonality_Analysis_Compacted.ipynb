{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nghitct/AlgorithmSupportedInductionPersonality/blob/main/TeamPersonality_Analysis_Compacted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WBE_a0lL39w"
      },
      "source": [
        "#Preparing ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jUh8PHIvqsX3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419d50a6-332c-4279-e5b9-8cad5039c583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#import basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.iolib.summary2 import summary_col\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#warnings.filterwarnings(action='once')\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "#import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn import tree\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#import google driver\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rKhLWVerH8U"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Kb4zfULvrkDg"
      },
      "outputs": [],
      "source": [
        "#import data\n",
        "dat1=pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/TeamPersonality-Data1-full.csv\")\n",
        "dat2=pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/TeamPersonality-Data2-full.csv\")\n",
        "\n",
        "dat1=dat1.drop(['gender_all_sd','gender_exc_sd','size','size_all','GroupID'],axis=1)\n",
        "dat2=dat2.drop(['gender_all_sd','gender_exc_sd','size','size_all','teamid'],axis=1)\n",
        "\n",
        "dat2_names=[re.sub('neur','emos',x) for x in dat2.columns]\n",
        "dat2.columns=dat2_names\n",
        "\n",
        "features_exc=['emos_exc_mean', 'extr_exc_mean',\n",
        "       'open_exc_mean', 'agree_exc_mean', 'cons_exc_mean', 'emos_exc_sd',\n",
        "       'extr_exc_sd', 'open_exc_sd', 'agree_exc_sd', 'cons_exc_sd',\n",
        "       'emos_exc_min', 'extr_exc_min', 'open_exc_min', 'agree_exc_min',\n",
        "       'cons_exc_min', 'emos_exc_max', 'extr_exc_max', 'open_exc_max',\n",
        "       'agree_exc_max', 'cons_exc_max', 'gender_exc_mean', 'gender_leader',\n",
        "       'emos_leader', 'extr_leader', 'open_leader', 'agree_leader',\n",
        "       'cons_leader']\n",
        "\n",
        "features_all=['emos_all_mean', 'extr_all_mean', 'open_all_mean', 'agree_all_mean',\n",
        "       'cons_all_mean', 'emos_all_sd', 'extr_all_sd', 'open_all_sd',\n",
        "       'agree_all_sd', 'cons_all_sd', 'emos_all_min', 'extr_all_min',\n",
        "       'open_all_min', 'agree_all_min', 'cons_all_min', 'emos_all_max',\n",
        "       'extr_all_max', 'open_all_max', 'agree_all_max', 'cons_all_max',\n",
        "       'gender_all_mean','gender_leader','emos_leader', 'extr_leader', \n",
        "       'open_leader', 'agree_leader','cons_leader']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define functions"
      ],
      "metadata": {
        "id": "rA6IC3_rEPm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _high_level_vars(data):\n",
        "  features=data.columns\n",
        "  m=len(features)\n",
        "  for i in range(0,(m-1)):\n",
        "    for j in range((i+1),m) :\n",
        "      var1=features[i]\n",
        "      var2=features[j]\n",
        "      name=var1+\"*\"+var2\n",
        "      data[name]=pd.Series(data[var1]*data[var2],name=name)  \n",
        "  for i in features:\n",
        "    name=i+\"2\"\n",
        "    data[name]=pd.Series(data[i]*data[i],name=name) \n",
        "  return data\n",
        "\n",
        "def _AIC_linearmodel(X, y, features):\n",
        "  model = sm.OLS(y, X[list(features)])\n",
        "  regr = model.fit()\n",
        "  AIC = regr.aic\n",
        "  return {'model':regr, 'AIC':AIC}\n",
        "\n",
        "def _forward(X, y, predictors):\n",
        "  remaining_predictors = [p for p in X.columns.difference(['const']) if p not in predictors]\n",
        "  results=[]\n",
        "  for p in remaining_predictors:\n",
        "    results.append(_AIC_linearmodel(X, y, features=predictors+[p]+['const']))\n",
        "  models = pd.DataFrame(results)\n",
        "  best_model = models.loc[models['AIC'].argmin()]\n",
        "  return best_model\n",
        "\n",
        "def _backward(X,y,predictors):\n",
        "  results = []\n",
        "  for combo in itertools.combinations(predictors, len(predictors) - 1):\n",
        "    results.append(_AIC_linearmodel(X=X, y= y,features=list(combo)+['const']))\n",
        "  models = pd.DataFrame(results)\n",
        "  best_model = models.loc[models['AIC'].argmin()]\n",
        "  return best_model\n",
        "\n",
        "def _stepwise_model(X,y):\n",
        "  Stepmodels = pd.DataFrame(columns=[\"AIC\", \"model\"])\n",
        "  predictors = []\n",
        "  Smodel_before = _AIC_linearmodel(X,y,predictors+['const'])['AIC']\n",
        "  for i in range(1, len(X.columns.difference(['const'])) + 1):\n",
        "    Forward_result = _forward(X=X, y=y, predictors=predictors) # constant added\n",
        "    #print('forward')\n",
        "    Stepmodels.loc[i] = Forward_result\n",
        "    predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n",
        "    predictors = [ k for k in predictors if k != 'const']\n",
        "    Backward_result = _backward(X=X, y=y, predictors=predictors)  # Check if there is anything to remove\n",
        "    if Backward_result['AIC']< Forward_result['AIC']:\n",
        "      Stepmodels.loc[i] = Backward_result\n",
        "      predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n",
        "      Smodel_before = Stepmodels.loc[i][\"AIC\"]\n",
        "      predictors = [ k for k in predictors if k != 'const']\n",
        "      #print('backward')\n",
        "    if Stepmodels.loc[i]['AIC']> Smodel_before:\n",
        "      break\n",
        "    else:\n",
        "      Smodel_before = Stepmodels.loc[i][\"AIC\"]\n",
        "    return (Stepmodels['model'][len(Stepmodels['model'])])\n",
        "\n",
        "def _fs_swAIC(X,y):\n",
        "  sw=_stepwise_model(X,y)\n",
        "  sw_p=sw.pvalues\n",
        "  features=sw_p[sw_p<=0.05].index.tolist()\n",
        "  features.remove('const') if 'const' in features else None\n",
        "  return features\n",
        "\n",
        "def _fs_randomforest(X,y,params,cv,n_iter,n_vars,randomstate):\n",
        "  #Create the model to tune\n",
        "  rf = RandomForestRegressor()\n",
        "  #search across different combinations, and use all available cores\n",
        "  rf_random = RandomizedSearchCV(estimator = rf, \n",
        "                                 param_distributions = params, \n",
        "                                 n_iter = n_iter, cv = cv, \n",
        "                                 verbose=2, \n",
        "                                 random_state=randomstate, \n",
        "                                 n_jobs = -1)\n",
        "  # Fit the random search model with exc variables\n",
        "  rf_random.fit(X,y)\n",
        "  best_params=rf_random.best_params_\n",
        "  rf_final = RandomForestRegressor(**best_params)\n",
        "  rf_final.fit(X, y)\n",
        "  features=X.columns\n",
        "  f_i = list(zip(features,rf_final.feature_importances_))\n",
        "  f_i.sort(key = lambda x : x[1],reverse=True)\n",
        "  features=[x[0] for x in f_i[0:n_vars]]\n",
        "  return features \n",
        "\n",
        "def _fs_gbr(X,y,params,cv,n_job,n_vars,randomstate):\n",
        "  gb = GradientBoostingRegressor()\n",
        "  gb_random = RandomizedSearchCV(estimator = gb,\n",
        "                                 param_distributions = params,\n",
        "                                 scoring = 'neg_mean_absolute_error',\n",
        "                                 n_iter = n_iter,\n",
        "                                 cv = cv,\n",
        "                                 refit = True,\n",
        "                                 return_train_score = True,\n",
        "                                 random_state = randomstate)\n",
        "  gb_random.fit(X,y)\n",
        "  best_params=gb_random.best_params_\n",
        "  gb_final=GradientBoostingRegressor(**best_params)\n",
        "  gb_final.fit(X,y)\n",
        "  features=X.columns\n",
        "  f_i = list(zip(features,gb_final.feature_importances_))\n",
        "  f_i.sort(key = lambda x : x[1],reverse=True)\n",
        "  features=[x[0] for x in f_i[0:n_vars]]\n",
        "  return features\n",
        "\n",
        "def _fs_lasso(X,y,cvparams,alphas,n_vars):\n",
        "  # define model evaluation method\n",
        "  cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=123456)\n",
        "  # define model\n",
        "  ls = LassoCV(alphas=alphas, cv=cv, n_jobs=-1)\n",
        "  # fit model\n",
        "  ls.fit(X,y)\n",
        "  # define model\n",
        "  ls_final = Lasso(alpha=ls.alpha_)\n",
        "  # fit model\n",
        "  ls_final.fit(X,y)\n",
        "  features=X.columns\n",
        "  f_i = list(zip(features,abs(ls_final.coef_)))\n",
        "  f_i.sort(key = lambda x : x[1],reverse=True)\n",
        "  features=[x[0] for x in f_i[0:n_vars]]\n",
        "  return features\n",
        "\n",
        "def _fs_ElasticNet(X,y,cvparams,ratios,alphas,n_vars):\n",
        "  cv = RepeatedKFold(**cvparams)\n",
        "  #ratios = np.arange(0, 1, 0.01)\n",
        "  #alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
        "  en = ElasticNetCV(l1_ratio=ratios, alphas=alphas, cv=cv, n_jobs=-1)\n",
        "  en.fit(X,y)\n",
        "  en_final = ElasticNet(alpha=en.alpha_, l1_ratio=en.l1_ratio_)\n",
        "  en_final.fit(X,y)\n",
        "  features=X.columns\n",
        "  f_i = list(zip(features,abs(en_final.coef_)))\n",
        "  f_i.sort(key = lambda x : x[1],reverse=True)\n",
        "  features=[x[0] for x in f_i[0:n_vars]]\n",
        "  return features\n",
        "\n",
        "def _test_hypothesis(X,y,predictors):\n",
        "  X_min=X[predictors]\n",
        "  X_min = sm.add_constant(X_min)\n",
        "  #fit linear regression model\n",
        "  model = sm.OLS(y, X_min).fit()\n",
        "  results = pd.DataFrame({'Coef':model.params[1:14],'p-value':model.pvalues[1:14]})\n",
        "  results['vars']=results.index\n",
        "  results.index=np.arange(0,len(predictors),1)\n",
        "  return results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eiGyHFO5ZFLG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xfElUiBj-8q"
      },
      "source": [
        "# Round 1: Random Forest + Lasso + 5 vars"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define paramaters"
      ],
      "metadata": {
        "id": "JOhi5FH5C7Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params for splitting\n",
        "randomstate_list=[1,10,100,200,300]\n",
        "testsize=[0.5,0.4,0.3,0.2]\n",
        "\n",
        "n_vars=5\n",
        "\n",
        "# params for random forest\n",
        "params = {'n_estimators': [int(x) for x in np.arange(10,300,10)],\n",
        "               'max_features': ['auto', 'sqrt'],\n",
        "               'max_depth': [3,5,10],\n",
        "               'min_samples_split': [2, 4, 6, 8, 10],\n",
        "               'min_samples_leaf': [1,2,4,6,8,10],\n",
        "               'bootstrap': [True, False]}\n",
        "cv=10\n",
        "n_iter=200\n",
        "\n",
        "# params for cv\n",
        "cvparams={'n_splits':10,\n",
        "          'n_repeats': 3,\n",
        "          'random_state': 123456}\n",
        "\n",
        "# params for lasso:\n",
        "ls_alphas=np.arange(0, 2, 0.1)\n",
        "\n",
        "# params for Elastic Net:\n",
        "en_ratios = np.arange(0, 1, 0.01)\n",
        "en_alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]"
      ],
      "metadata": {
        "id": "cJbIUdyTuaSu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Induction and Testing"
      ],
      "metadata": {
        "id": "fYlGrDMvC-F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_all = pd.DataFrame(columns=['Coef','p-value','vars','randomstate','testsize'])\n",
        "for rs in randomstate_list:\n",
        "  for ts in testsize:\n",
        "    train,test=train_test_split(dat1,test_size=ts,random_state=rs)\n",
        "    train.shape\n",
        "    test.shape\n",
        "\n",
        "    X1_train_exc=train[features_exc]\n",
        "    Y1_train=train['performance']\n",
        "    X1_test_exc=test[features_exc]\n",
        "    Y1_test=test['performance']\n",
        "\n",
        "    rf_features=_fs_randomforest(X1_train_exc,Y1_train,params,cv,n_iter,n_vars,123456)\n",
        "    X1_train_exc_poly=_high_level_vars(X1_train_exc[rf_features])\n",
        "    \n",
        "    ls_predictors=_fs_lasso(X1_train_exc_poly,Y1_train,cvparams,ls_alphas,n_vars)\n",
        "    \n",
        "    X1_test_exc_poly=_high_level_vars(X1_test_exc[rf_features])\n",
        "    results=_test_hypothesis(X1_test_exc_poly,Y1_test,ls_predictors)\n",
        "\n",
        "    results['randomstate']=rs\n",
        "    results['testsize']=ts\n",
        "    \n",
        "    results_all = pd.concat([results_all,results],ignore_index=True)\n",
        "\n",
        "path=\"/content/gdrive/MyDrive/Colab Notebooks/TeamPersonality-rf-lasso-5-sim1221-1.csv\"\n",
        "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "  results_all.to_csv(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwnK3Vzvxjx2",
        "outputId": "7f9c9b55-8477-4707-b1d9-e465b9f38bd7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Round 2: Random Forest + Elastic Net + 5 vars"
      ],
      "metadata": {
        "id": "fYhOzKa3uRz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define parameters"
      ],
      "metadata": {
        "id": "WAhE4XwdubB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params for splitting\n",
        "randomstate_list=[1,10,100,200,300]\n",
        "testsize=[0.5,0.4,0.3,0.2]\n",
        "\n",
        "n_vars=5\n",
        "\n",
        "# params for random forest\n",
        "params = {'n_estimators': [int(x) for x in np.arange(10,300,10)],\n",
        "               'max_features': ['auto', 'sqrt'],\n",
        "               'max_depth': [3,5,10],\n",
        "               'min_samples_split': [2, 4, 6, 8, 10],\n",
        "               'min_samples_leaf': [1,2,4,6,8,10],\n",
        "               'bootstrap': [True, False]}\n",
        "cv=10\n",
        "n_iter=200\n",
        "\n",
        "# params for cv\n",
        "cvparams={'n_splits':10,\n",
        "          'n_repeats': 3,\n",
        "          'random_state': 123456}\n",
        "\n",
        "# params for lasso:\n",
        "ls_alphas=np.arange(0, 2, 0.1)\n",
        "\n",
        "# params for Elastic Net:\n",
        "en_ratios = np.arange(0, 1, 0.01)\n",
        "en_alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]"
      ],
      "metadata": {
        "id": "7150lYh7uXnO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Induction and Testing\n",
        "\n"
      ],
      "metadata": {
        "id": "0VQ9SIIcufp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_all = pd.DataFrame(columns=['Coef','p-value','vars','randomstate','testsize'])\n",
        "for rs in randomstate_list:\n",
        "  for ts in testsize:\n",
        "    train,test=train_test_split(dat1,test_size=ts,random_state=rs)\n",
        "    train.shape\n",
        "    test.shape\n",
        "\n",
        "    X1_train_exc=train[features_exc]\n",
        "    Y1_train=train['performance']\n",
        "    X1_test_exc=test[features_exc]\n",
        "    Y1_test=test['performance']\n",
        "\n",
        "    rf_features=_fs_randomforest(X1_train_exc,Y1_train,params,cv,n_iter,n_vars,123456)\n",
        "    X1_train_exc_poly=_high_level_vars(X1_train_exc[rf_features])\n",
        "    \n",
        "    ls_predictors=_fs_ElasticNet(X1_train_exc_poly,Y1_train,cvparams,en_ratios,en_alphas,n_vars)\n",
        "    \n",
        "    X1_test_exc_poly=_high_level_vars(X1_test_exc[rf_features])\n",
        "    results=_test_hypothesis(X1_test_exc_poly,Y1_test,ls_predictors)\n",
        "\n",
        "    results['randomstate']=rs\n",
        "    results['testsize']=ts\n",
        "    \n",
        "    results_all = pd.concat([results_all,results],ignore_index=True)\n",
        "\n",
        "path=\"/content/gdrive/MyDrive/Colab Notebooks/TeamPersonality-rf-en-5-sim1221-1.csv\"\n",
        "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "  results_all.to_csv(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BpB7L52u4m3",
        "outputId": "53ccd7da-4a10-475f-e017-b3e596702bdc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Round 3: Gradient Boosting + Lasso + 5 vars"
      ],
      "metadata": {
        "id": "hXEPOzaXHZoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define parameters"
      ],
      "metadata": {
        "id": "hOjQfnPEHeEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params for splitting\n",
        "randomstate_list=[1,10,100,200,300]\n",
        "testsize=[0.5,0.4,0.3,0.2]\n",
        "\n",
        "n_vars=5\n",
        "\n",
        "# params for Gradient Boosting\n",
        "params = {'n_estimators': [int(x) for x in np.arange(10,300,10)],\n",
        "          'max_depth': [3,5,10],\n",
        "          'min_samples_split': [2,4,6,8,10],\n",
        "          'min_samples_leaf': [1,2,4,6,8,10],\n",
        "          'learning_rate': [x for x in np.arange(0.1,1,0.1)],\n",
        "          'criterion': ['friedman_mse','squared_error']}\n",
        "cv=10\n",
        "n_iter=200\n",
        "\n",
        "# params for cv\n",
        "cvparams={'n_splits':10,\n",
        "          'n_repeats': 3,\n",
        "          'random_state': 123456}\n",
        "\n",
        "# params for lasso:\n",
        "ls_alphas=np.arange(0, 2, 0.1)\n",
        "\n",
        "# params for Elastic Net:\n",
        "en_ratios = np.arange(0, 1, 0.01)\n",
        "en_alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]"
      ],
      "metadata": {
        "id": "bdt1-n88Hfh1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Induction and Testing"
      ],
      "metadata": {
        "id": "WFSHw36r7Uh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_all = pd.DataFrame(columns=['Coef','p-value','vars','randomstate','testsize'])\n",
        "for rs in randomstate_list:\n",
        "  for ts in testsize:\n",
        "    train,test=train_test_split(dat1,test_size=ts,random_state=rs)\n",
        "    train.shape\n",
        "    test.shape\n",
        "\n",
        "    X1_train_exc=train[features_exc]\n",
        "    Y1_train=train['performance']\n",
        "    X1_test_exc=test[features_exc]\n",
        "    Y1_test=test['performance']\n",
        "\n",
        "    rf_features=_fs_gbr(X1_train_exc,Y1_train,params,cv,n_iter,n_vars,123456)\n",
        "    X1_train_exc_poly=_high_level_vars(X1_train_exc[rf_features])\n",
        "    \n",
        "    ls_predictors=_fs_lasso(X1_train_exc_poly,Y1_train,cvparams,ls_alphas,n_vars)\n",
        "    \n",
        "    X1_test_exc_poly=_high_level_vars(X1_test_exc[rf_features])\n",
        "    results=_test_hypothesis(X1_test_exc_poly,Y1_test,ls_predictors)\n",
        "\n",
        "    results['randomstate']=rs\n",
        "    results['testsize']=ts\n",
        "    \n",
        "    results_all = pd.concat([results_all,results],ignore_index=True)\n",
        "\n",
        "path=\"/content/gdrive/MyDrive/Colab Notebooks/TeamPersonality-gbr-lasso-5-sim1221-1.csv\"\n",
        "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "  results_all.to_csv(f)"
      ],
      "metadata": {
        "id": "3bGHB8hgMJhv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Round 4: Random Forest + Lasso + 10 vars"
      ],
      "metadata": {
        "id": "3a7PNp4okFtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define parameters"
      ],
      "metadata": {
        "id": "yfB_rBBCkKYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params for splitting\n",
        "randomstate_list=[1,10,100,200,300]\n",
        "testsize=[0.5,0.4,0.3,0.2]\n",
        "\n",
        "n_vars=10\n",
        "\n",
        "# params for random forest\n",
        "params = {'n_estimators': [int(x) for x in np.arange(10,300,10)],\n",
        "               'max_features': ['auto', 'sqrt'],\n",
        "               'max_depth': [3,5,10],\n",
        "               'min_samples_split': [2, 4, 6, 8, 10],\n",
        "               'min_samples_leaf': [1,2,4,6,8,10],\n",
        "               'bootstrap': [True, False]}\n",
        "cv=10\n",
        "n_iter=200\n",
        "\n",
        "# params for cv\n",
        "cvparams={'n_splits':10,\n",
        "          'n_repeats': 3,\n",
        "          'random_state': 123456}\n",
        "\n",
        "# params for lasso:\n",
        "ls_alphas=np.arange(0, 2, 0.1)\n",
        "\n",
        "# params for Elastic Net:\n",
        "en_ratios = np.arange(0, 1, 0.01)\n",
        "en_alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]"
      ],
      "metadata": {
        "id": "9I3uqKenkLxI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Induction and Testing"
      ],
      "metadata": {
        "id": "Ew0bDdFFkRSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_all = pd.DataFrame(columns=['Coef','p-value','vars','randomstate','testsize'])\n",
        "for rs in randomstate_list:\n",
        "  for ts in testsize:\n",
        "    train,test=train_test_split(dat1,test_size=ts,random_state=rs)\n",
        "    train.shape\n",
        "    test.shape\n",
        "\n",
        "    X1_train_exc=train[features_exc]\n",
        "    Y1_train=train['performance']\n",
        "    X1_test_exc=test[features_exc]\n",
        "    Y1_test=test['performance']\n",
        "\n",
        "    rf_features=_fs_randomforest(X1_train_exc,Y1_train,params,cv,n_iter,n_vars,123456)\n",
        "    X1_train_exc_poly=_high_level_vars(X1_train_exc[rf_features])\n",
        "    \n",
        "    ls_predictors=_fs_lasso(X1_train_exc_poly,Y1_train,cvparams,ls_alphas,n_vars)\n",
        "    \n",
        "    X1_test_exc_poly=_high_level_vars(X1_test_exc[rf_features])\n",
        "    results=_test_hypothesis(X1_test_exc_poly,Y1_test,ls_predictors)\n",
        "\n",
        "    results['randomstate']=rs\n",
        "    results['testsize']=ts\n",
        "    \n",
        "    results_all = pd.concat([results_all,results],ignore_index=True)\n",
        "\n",
        "path=\"/content/gdrive/MyDrive/Colab Notebooks/TeamPersonality-rf-lasso-10-sim1223-1.csv\"\n",
        "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "  results_all.to_csv(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LbLk69ZkUjy",
        "outputId": "3cad8e3c-4a4c-4b58-e229-c132c64d97bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Round 5: Random Forest + Lasso + 5 vars + X2 as test"
      ],
      "metadata": {
        "id": "QqvO5qE2SoOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define parameters"
      ],
      "metadata": {
        "id": "DgUs4S71SykL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params for splitting\n",
        "randomstate_list=[1,10,100,200,300]\n",
        "testsize=[0.5,0.4,0.3,0.2]\n",
        "\n",
        "n_vars=5\n",
        "\n",
        "# params for random forest\n",
        "params = {'n_estimators': [int(x) for x in np.arange(10,300,10)],\n",
        "               'max_features': ['auto', 'sqrt'],\n",
        "               'max_depth': [3,5,10],\n",
        "               'min_samples_split': [2, 4, 6, 8, 10],\n",
        "               'min_samples_leaf': [1,2,4,6,8,10],\n",
        "               'bootstrap': [True, False]}\n",
        "cv=10\n",
        "n_iter=200\n",
        "\n",
        "# params for cv\n",
        "cvparams={'n_splits':10,\n",
        "          'n_repeats': 3,\n",
        "          'random_state': 123456}\n",
        "\n",
        "# params for lasso:\n",
        "ls_alphas=np.arange(0, 2, 0.1)\n",
        "\n",
        "# params for Elastic Net:\n",
        "en_ratios = np.arange(0, 1, 0.01)\n",
        "en_alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]"
      ],
      "metadata": {
        "id": "hbGQnz6bSucy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Induction and Testing"
      ],
      "metadata": {
        "id": "FQ0-PEm_S0uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train=dat1\n",
        "test=dat2\n",
        "\n",
        "X1_train_exc=train[features_exc]\n",
        "Y1_train=train['performance']\n",
        "X1_test_exc=test[features_exc]\n",
        "Y1_test=test['performance']\n",
        "\n",
        "rf_features=_fs_randomforest(X1_train_exc,Y1_train,params,cv,n_iter,n_vars,123456)\n",
        "X1_train_exc_poly=_high_level_vars(X1_train_exc[rf_features])\n",
        "    \n",
        "ls_predictors=_fs_lasso(X1_train_exc_poly,Y1_train,cvparams,ls_alphas,n_vars)\n",
        "  \n",
        "X1_test_exc_poly=_high_level_vars(X1_test_exc[rf_features])\n",
        "results=_test_hypothesis(X1_test_exc_poly,Y1_test,ls_predictors)    \n",
        "\n",
        "path=\"/content/gdrive/MyDrive/Colab Notebooks/TeamPersonality-rf-lasso-5-Dat2AsTest.csv\"\n",
        "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "  results.to_csv(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7q_CsF-S2R-",
        "outputId": "8be8703c-c07a-4c15-e416-333c4b0af2dd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Round 6: Random Forest + Lasso + 10 vars + X2 as test"
      ],
      "metadata": {
        "id": "EYLw9jr2XQCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define parameters"
      ],
      "metadata": {
        "id": "uJGVOKzFXY27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params for splitting\n",
        "randomstate_list=[1,10,100,200,300]\n",
        "testsize=[0.5,0.4,0.3,0.2]\n",
        "\n",
        "n_vars=10\n",
        "\n",
        "# params for random forest\n",
        "params = {'n_estimators': [int(x) for x in np.arange(10,300,10)],\n",
        "               'max_features': ['auto', 'sqrt'],\n",
        "               'max_depth': [3,5,10],\n",
        "               'min_samples_split': [2, 4, 6, 8, 10],\n",
        "               'min_samples_leaf': [1,2,4,6,8,10],\n",
        "               'bootstrap': [True, False]}\n",
        "cv=10\n",
        "n_iter=200\n",
        "\n",
        "# params for cv\n",
        "cvparams={'n_splits':10,\n",
        "          'n_repeats': 3,\n",
        "          'random_state': 123456}\n",
        "\n",
        "# params for lasso:\n",
        "ls_alphas=np.arange(0, 2, 0.1)\n",
        "\n",
        "# params for Elastic Net:\n",
        "en_ratios = np.arange(0, 1, 0.01)\n",
        "en_alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]"
      ],
      "metadata": {
        "id": "meiRrhKeXUuC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Induction and Testing"
      ],
      "metadata": {
        "id": "3_cuGB94Xa-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train=dat1\n",
        "test=dat2\n",
        "\n",
        "X1_train_exc=train[features_exc]\n",
        "Y1_train=train['performance']\n",
        "X1_test_exc=test[features_exc]\n",
        "Y1_test=test['performance']\n",
        "\n",
        "rf_features=_fs_randomforest(X1_train_exc,Y1_train,params,cv,n_iter,n_vars,123456)\n",
        "X1_train_exc_poly=_high_level_vars(X1_train_exc[rf_features])\n",
        "    \n",
        "ls_predictors=_fs_lasso(X1_train_exc_poly,Y1_train,cvparams,ls_alphas,n_vars)\n",
        "  \n",
        "X1_test_exc_poly=_high_level_vars(X1_test_exc[rf_features])\n",
        "results=_test_hypothesis(X1_test_exc_poly,Y1_test,ls_predictors)    \n",
        "\n",
        "path=\"/content/gdrive/MyDrive/Colab Notebooks/TeamPersonality-rf-lasso-10-Dat2AsTest.csv\"\n",
        "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "  results.to_csv(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wrx8xxFhXcKD",
        "outputId": "c5ce89f0-9ae5-431d-8830-365ba1f4ab91"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Round 7: Random Forest + Elastic Net + 5 vars + X2 as test"
      ],
      "metadata": {
        "id": "xFQow95-Xj5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define parameters"
      ],
      "metadata": {
        "id": "OfpQV3rCXzJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params for splitting\n",
        "randomstate_list=[1,10,100,200,300]\n",
        "testsize=[0.5,0.4,0.3,0.2]\n",
        "\n",
        "n_vars=5\n",
        "\n",
        "# params for random forest\n",
        "params = {'n_estimators': [int(x) for x in np.arange(10,300,10)],\n",
        "               'max_features': ['auto', 'sqrt'],\n",
        "               'max_depth': [3,5,10],\n",
        "               'min_samples_split': [2, 4, 6, 8, 10],\n",
        "               'min_samples_leaf': [1,2,4,6,8,10],\n",
        "               'bootstrap': [True, False]}\n",
        "cv=10\n",
        "n_iter=200\n",
        "\n",
        "# params for cv\n",
        "cvparams={'n_splits':10,\n",
        "          'n_repeats': 3,\n",
        "          'random_state': 123456}\n",
        "\n",
        "# params for lasso:\n",
        "ls_alphas=np.arange(0, 2, 0.1)\n",
        "\n",
        "# params for Elastic Net:\n",
        "en_ratios = np.arange(0, 1, 0.01)\n",
        "en_alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]"
      ],
      "metadata": {
        "id": "rzgWMTE3X2e7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Induction and Testing"
      ],
      "metadata": {
        "id": "0HiQqjIIX0d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train=dat1\n",
        "test=dat2\n",
        "\n",
        "X1_train_exc=train[features_exc]\n",
        "Y1_train=train['performance']\n",
        "X1_test_exc=test[features_exc]\n",
        "Y1_test=test['performance']\n",
        "\n",
        "rf_features=_fs_randomforest(X1_train_exc,Y1_train,params,cv,n_iter,n_vars,123456)\n",
        "X1_train_exc_poly=_high_level_vars(X1_train_exc[rf_features])\n",
        "    \n",
        "ls_predictors=_fs_ElasticNet(X1_train_exc_poly,Y1_train,cvparams,en_ratios,en_alphas,n_vars)\n",
        "  \n",
        "X1_test_exc_poly=_high_level_vars(X1_test_exc[rf_features])\n",
        "results=_test_hypothesis(X1_test_exc_poly,Y1_test,ls_predictors)    \n",
        "\n",
        "path=\"/content/gdrive/MyDrive/Colab Notebooks/TeamPersonality-rf-en-5-Dat2AsTest.csv\"\n",
        "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "  results.to_csv(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtsGGG3pX9gN",
        "outputId": "416374af-e9d9-4918-daca-4765a09d82d9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Round 8: Gradient Boosting + Lasso + 5 vars + X2 as test"
      ],
      "metadata": {
        "id": "oPLhGA7qYNoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define parameters"
      ],
      "metadata": {
        "id": "PY-SNO4AYSTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params for splitting\n",
        "randomstate_list=[1,10,100,200,300]\n",
        "testsize=[0.5,0.4,0.3,0.2]\n",
        "\n",
        "n_vars=5\n",
        "\n",
        "# params for Gradient Boosting\n",
        "params = {'n_estimators': [int(x) for x in np.arange(10,300,10)],\n",
        "          'max_depth': [3,5,10],\n",
        "          'min_samples_split': [2,4,6,8,10],\n",
        "          'min_samples_leaf': [1,2,4,6,8,10],\n",
        "          'learning_rate': [x for x in np.arange(0.1,1,0.1)],\n",
        "          'criterion': ['friedman_mse','squared_error']}\n",
        "cv=10\n",
        "n_iter=200\n",
        "\n",
        "# params for cv\n",
        "cvparams={'n_splits':10,\n",
        "          'n_repeats': 3,\n",
        "          'random_state': 123456}\n",
        "\n",
        "# params for lasso:\n",
        "ls_alphas=np.arange(0, 2, 0.1)\n",
        "\n",
        "# params for Elastic Net:\n",
        "en_ratios = np.arange(0, 1, 0.01)\n",
        "en_alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]"
      ],
      "metadata": {
        "id": "QdpQJ_epYVUc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Induction and Testing"
      ],
      "metadata": {
        "id": "b3kj-BrbYThl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train=dat1\n",
        "test=dat2\n",
        "\n",
        "X1_train_exc=train[features_exc]\n",
        "Y1_train=train['performance']\n",
        "X1_test_exc=test[features_exc]\n",
        "Y1_test=test['performance']\n",
        "\n",
        "rf_features=_fs_gbr(X1_train_exc,Y1_train,params,cv,n_iter,n_vars,123456)\n",
        "X1_train_exc_poly=_high_level_vars(X1_train_exc[rf_features])\n",
        "    \n",
        "ls_predictors=_fs_lasso(X1_train_exc_poly,Y1_train,cvparams,ls_alphas,n_vars)\n",
        "  \n",
        "X1_test_exc_poly=_high_level_vars(X1_test_exc[rf_features])\n",
        "results=_test_hypothesis(X1_test_exc_poly,Y1_test,ls_predictors)    \n",
        "\n",
        "path=\"/content/gdrive/MyDrive/Colab Notebooks/TeamPersonality-gbr-lasso-5-Dat2AsTest.csv\"\n",
        "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "  results.to_csv(f)"
      ],
      "metadata": {
        "id": "vDJuFqm3YZpR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Side Rounds: Select poly-vars directly"
      ],
      "metadata": {
        "id": "we73UtjvFU8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define parameters"
      ],
      "metadata": {
        "id": "mE3mzxF7Hs-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params for splitting\n",
        "randomstate_list=[1,10,100,200,300]\n",
        "testsize=[0.5,0.4,0.3,0.2]\n",
        "\n",
        "n_vars=10\n",
        "\n",
        "# params for cv\n",
        "cvparams={'n_splits':10,\n",
        "          'n_repeats': 3,\n",
        "          'random_state': 123456}\n",
        "\n",
        "# params for lasso:\n",
        "ls_alphas=np.arange(0, 2, 0.1)"
      ],
      "metadata": {
        "id": "XzzM8wcoHR7s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso model"
      ],
      "metadata": {
        "id": "nSGcI5vVHj63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_all = pd.DataFrame(columns=['Coef','p-value','vars','randomstate','testsize'])\n",
        "for rs in randomstate_list:\n",
        "  for ts in testsize:\n",
        "    train,test=train_test_split(dat1,test_size=ts,random_state=rs)\n",
        "    train.shape\n",
        "    test.shape\n",
        "\n",
        "    X1_train_exc=train[features_exc]\n",
        "    Y1_train=train['performance']\n",
        "    X1_test_exc=test[features_exc]\n",
        "    Y1_test=test['performance']\n",
        "\n",
        "    X1_train_exc_poly=_high_level_vars(X1_train_exc)\n",
        "    \n",
        "    ls_predictors=_fs_lasso(X1_train_exc_poly,Y1_train,cvparams,ls_alphas,n_vars)\n",
        "    \n",
        "    X1_test_exc_poly=_high_level_vars(X1_test_exc)\n",
        "    results=_test_hypothesis(X1_test_exc_poly,Y1_test,ls_predictors)\n",
        "\n",
        "    results['randomstate']=rs\n",
        "    results['testsize']=ts\n",
        "    \n",
        "    results_all = pd.concat([results_all,results],ignore_index=True)\n",
        "\n",
        "path=\"/content/gdrive/MyDrive/Colab Notebooks/TeamPersonality-direct-lasso-sim1222-1.csv\"\n",
        "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "  results_all.to_csv(f)"
      ],
      "metadata": {
        "id": "_d8OvI7CFUGz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stepwise model"
      ],
      "metadata": {
        "id": "u1HfgR7tHpvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_all = pd.DataFrame(columns=['Coef','p-value','vars','randomstate','testsize'])\n",
        "for rs in randomstate_list:\n",
        "  for ts in testsize:\n",
        "    train,test=train_test_split(dat1,test_size=ts,random_state=rs)\n",
        "    train.shape\n",
        "    test.shape\n",
        "\n",
        "    X1_train_exc=train[features_exc]\n",
        "    Y1_train=train['performance']\n",
        "    X1_test_exc=test[features_exc]\n",
        "    Y1_test=test['performance']\n",
        "\n",
        "    X1_train_exc_poly=_high_level_vars(X1_train_exc)\n",
        "    X1_train_exc_poly['const']=1\n",
        "    ls_predictors=_fs_swAIC(X1_train_exc_poly,Y1_train)\n",
        "    \n",
        "    X1_test_exc_poly=_high_level_vars(X1_test_exc)\n",
        "    results=_test_hypothesis(X1_test_exc_poly,Y1_test,ls_predictors)\n",
        "\n",
        "    results['randomstate']=rs\n",
        "    results['testsize']=ts\n",
        "    \n",
        "    results_all = pd.concat([results_all,results],ignore_index=True)\n",
        "\n",
        "path=\"/content/gdrive/MyDrive/Colab Notebooks/TeamPersonality-direct-sw-sim1222-1.csv\"\n",
        "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "  results_all.to_csv(f)"
      ],
      "metadata": {
        "id": "xkR8mnEnG5GL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y4k9Qe9hFUZC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "-WBE_a0lL39w",
        "8rKhLWVerH8U",
        "0rJQUWbMw-Jz",
        "q5gVSKPOxJQI",
        "JzZtr9fpxOxn",
        "rBsA2FMT_Kp5",
        "7DMNU2s5xor9",
        "rAwfDnOxr6Q8",
        "_s94VSU9MCHW",
        "UtRZO5wEU5-0",
        "klIlo6kGWiNB",
        "ctHo93yPXRkc",
        "Gq6nMC2kZMFc",
        "HUxMQ-VjZ9mt",
        "zXzrWDOYax82",
        "gwQasJC0bJ_E",
        "lWpw1JdDb5XZ",
        "WPyh8e8Udnxv",
        "LPzvlRS0dw9k",
        "i6clmPJGeYhD",
        "TEnPiSmNegoo",
        "tEgIfk0He_FZ",
        "gJqMbPQkf5xn",
        "HxnPOjFLgRJ-",
        "X3RrsR6MggTF",
        "067D5KLtgzyl",
        "U0jdhvMWxYWB",
        "O8NW5YQo5dJM",
        "prgFjPkiYkwV"
      ],
      "mount_file_id": "1UB7JCFP4QSQJak1dn_w_jh5RUAFq0G6E",
      "authorship_tag": "ABX9TyM8kkGr70cFEhGpirpkaxLE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}